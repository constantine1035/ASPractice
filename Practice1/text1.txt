
Энтропия
Материал из Википедии — свободной энциклопедии
Текущая версия страницы пока не проверялась опытными участниками и может значительно отличаться от версии, проверенной 25 августа 2022 года; проверки требуют 60 правок.
У этого термина существуют и другие значения, см. Энтропия (значения).

Согласно второму закону термодинамики, энтропия изолированной системы возрастает или остается постоянной. Можно сказать, что поведение энтропии указывает на направление времени (aika)[1].
Энтропи́я (от др.-греч. ἐν «в» + τροπή «обращение; превращение») — широко используемый в естественных и точных науках термин (впервые введён в рамках термодинамики как функция состояния термодинамической системы), обозначающий меру необратимого рассеивания энергии или бесполезности энергии (потому что не всю энергию системы можно использовать для превращения в какую-нибудь полезную работу)[2].
Для понятия энтропии в данном разделе физики используют название термодинамическая энтропия; термодинамическая энтропия обычно применяется для описания равновесных (обратимых) процессов.
В статистической физике энтропия характеризует вероятность осуществления какого-либо макроскопического состояния. Кроме физики, термин широко употребляется в математике: теории информации и математической статистике. В этих областях знания энтропия определяется статистически и называется статистической или информационной энтропией. Данное определение энтропии известно также как энтропия Шеннона (в математике) и энтропия Больцмана—Гиббса (в физике).
Хотя понятия термодинамической и информационной энтропии вводятся в рамках различных формализмов, они имеют общий физический смысл — логарифм числа доступных состояний системы. Взаимосвязь этих понятий впервые установил Людвиг Больцман. В неравновесных (необратимых) процессах энтропия также служит мерой близости состояния системы к равновесному: чем больше энтропия, тем ближе система к равновесию (в состоянии термодинамического равновесия энтропия системы максимальна).
Величина, противоположная энтропии, именуется негэнтропией или, реже, экстропией.

Содержание
1	Употребление в различных дисциплинах
2	В термодинамике
2.1	Физический смысл энтропии
3	В теории информации
3.1	Аксиоматическое определение энтропии
3.2	f-энтропия
4	В биологии
5	См. также
6	Примечания
7	Литература
8	Ссылки
Употребление в различных дисциплинах
Термодинамическая энтропия — термодинамическая функция, характеризующая меру необратимой диссипации энергии в ней.
В статистической физике — характеризует вероятность осуществления некоторого макроскопического состояния системы.
В математической статистике — мера неопределённости распределения вероятностей.
Информационная энтропия — в теории информации мера неопределённости источника сообщений, определяемая вероятностями появления тех или иных символов при их передаче.
Энтропия динамической системы — в теории динамических систем мера хаотичности в поведении траекторий системы.
Дифференциальная энтропия — формальное обобщение понятия энтропии для непрерывных распределений.
Энтропия отражения — часть информации о дискретной системе, которая не воспроизводится при отражении системы через совокупность своих частей.
Энтропия в теории управления — мера неопределённости состояния или поведения системы в данных условиях.
Технологическая энтропия — «количественная мера отставания данной технологии от наивысшего в мире уровня, принимаемого за единицу. Основным параметром, характеризующим меру технологической энтропии, является степень неопределённости получения конечного результата, а именно: объёма выпуска инновационной продукции, который может быть получен при данных объёмах вовлечённых в производство ресурсов»[3].
В термодинамике
Основная статья: Энтропия в классической термодинамике

Рудольф Клаузиус
Понятие энтропии впервые было введено Клаузиусом в термодинамике в 1865 году для определения меры необратимого рассеивания энергии, меры отклонения реального процесса от идеального. Определённая как сумма приведённых теплот, она является функцией состояния и остаётся постоянной при замкнутых обратимых процессах, тогда как в необратимых замкнутых — её изменение всегда положительно. В открытой системе может происходить уменьшение энтропии рассматриваемой системы за счет уноса энергии, например, в виде излучения, при этом полная энтропия окружающей среды увеличивается[4].
Математически энтропия определяется как функция состояния системы, определённая с точностью до произвольного постоянного слагаемого. Разность энтропий в двух равновесных состояниях 1 и 2 по определению равна приведённому количеству тепла (
δ
Q
/
T
{\displaystyle \delta Q/T}), которое надо сообщить системе, чтобы перевести её из состояния 1 в состояние 2 по любому квазистатическому пути[5]:
Δ
S
1
→
2
=
S
2
−
S
1
=
∫
1
→
2
δ
Q
T
{\displaystyle \Delta S_{1\to 2}=S_{2}-S_{1}=\int \limits _{1\to 2}{\frac {\delta Q}{T}}}.	(1)
Так как энтропия определена с точностью до произвольной аддитивной постоянной, то можно условно принять состояние 1 за начальное и положить
S
1
=
0
{\displaystyle S_{1}=0}. Тогда
S
=
∫
δ
Q
T
{\displaystyle S=\int {\frac {\delta Q}{T}}},	(2)
Здесь интеграл берется для произвольного квазистатического процесса. Дифференциал функции
S
{\displaystyle S} имеет вид
d
S
=
δ
Q
T
{\displaystyle dS={\frac {\delta Q}{T}}}.	(3)
Энтропия устанавливает связь между макро- и микросостояниями. Особенность данной характеристики заключается в том, что это единственная функция в физике, которая показывает направленность процессов. Поскольку энтропия является функцией состояния, то она не зависит от того, как осуществлён переход из одного состояния системы в другое, а определяется только начальным и конечным состояниями системы.
Физический смысл энтропии
Термодинамическая энтропия как физическая величина отличается своей абстрактностью, физический смысл энтропии непосредственно не вытекает из её математического выражения и не поддаётся простому интуитивному восприятию.
С физической точки зрения энтропия характеризует степень необратимости, неидеальности реального термодинамического процесса. Она является мерой диссипации (рассеивания) энергии, а также мерой оценки энергии в плане её пригодности (или эффективности) использования для превращения теплоты в работу. [6] Два последних утверждения не относятся к необычным системам с отрицательной абсолютной температурой, в которых теплота самопроизвольно может полностью превращаться в работу.
В теории информации
Основная статья: Информационная энтропия
Для энтропии (чаще в математике) встречается также название шенноновская информация или количество информации по Шеннону[7].
Энтропия может интерпретироваться как мера неопределённости (неупорядоченности) или сложности некоторой системы, например, какого-либо опыта (испытания), который может иметь разные исходы, а значит, и количество информации[8][9]. Таким образом, другой интерпретацией энтропии является информационная ёмкость системы. С данной интерпретацией связан тот факт, что создатель понятия энтропии в теории информации (Клод Шеннон) сначала хотел назвать эту величину информацией.
Понятие информационной энтропии применяется как в теории информации и математической статистике, так и в статистической физике (энтропия Гиббса и её упрощённый вариант — энтропия Больцмана)[10][11]. Математический смысл информационной энтропии — это логарифм числа доступных состояний системы (основание логарифма может быть различным, но большим 1, оно определяет единицу измерения энтропии)[12]. Такая функция от числа состояний обеспечивает свойство аддитивности энтропии для независимых систем. Причём, если состояния различаются по степени доступности (то есть не равновероятны), под числом состояний системы нужно понимать их эффективное количество, которое определяется следующим образом.
Пусть состояния системы равновероятны и имеют вероятность
p
{\displaystyle p}, тогда число состояний
N
=
1
/
p
{\displaystyle N=1/p}, а
log
⁡
N
=
log
⁡
(
1
/
p
)
{\displaystyle \log N=\log(1/p)}. В случае разных вероятностей состояний
p
i
{\displaystyle p_{i}} рассмотрим средневзвешенную величину
log
⁡
N
¯
=
∑
i
=
1
N
p
i
log
⁡
(
1
/
p
i
)
,
{\displaystyle \log {\overline {N}}=\sum _{i=1}^{N}p_{i}\log(1/p_{i}),}
где
N
¯
{\displaystyle {\overline {N}}} — эффективное количество состояний. Из данной интерпретации непосредственно вытекает выражение для информационной энтропии Шеннона:
H
=
log
⁡
N
¯
=
−
∑
i
=
1
N
p
i
log
⁡
p
i
.
{\displaystyle H=\log {\overline {N}}=-\sum _{i=1}^{N}p_{i}\log p_{i}.}
Подобная интерпретация справедлива и для энтропии Реньи, которая является одним из обобщений понятия информационная энтропия, но в этом случае иначе определяется эффективное количество состояний системы. Энтропии Реньи соответствует эффективное количество состояний, определяемое[13] как среднее степенное взвешенное с параметром
q
≤
1
{\displaystyle q\leq 1} от величин
1
/
p
i
{\displaystyle 1/p_{i}}.
Следует заметить, что интерпретация формулы Шеннона на основе взвешенного среднего не является её обоснованием. Строгий вывод этой формулы может быть получен из комбинаторных соображений с помощью асимптотической формулы Стирлинга и заключается в том, что комбинаторность распределения (то есть число способов, которыми оно может быть реализовано) после взятия логарифма и нормировки в пределе совпадает с выражением для энтропии в виде, предложенном Шенноном[14][15][16].
Аксиоматическое определение энтропии
Выражение для информационной энтропии может быть выведено на основе некоторой системы аксиом. Одним из подходов является следующая система аксиом, известная как система аксиом Хинчина:[17].
1. Пусть некоторая система может пребывать в каждом из
N
{\displaystyle N} доступных состояний с вероятностью
p
i
{\displaystyle p_{i}}, где
i
=
1
,
.
.
.
,
N
{\displaystyle i=1,...,N}. Энтропия
H
{\displaystyle H} является функцией только вероятностей
P
=
(
p
1
,
.
.
.
,
p
N
)
{\displaystyle P=(p_{1},...,p_{N})}:
H
=
H
(
P
)
{\displaystyle H=H(P)}.
2. Для любой системы
P
{\displaystyle P} справедливо
H
(
P
)
≤
H
(
P
u
n
i
f
)
{\displaystyle H(P)\leq H(P_{unif})}, где
P
u
n
i
f
{\displaystyle P_{unif}} — система с равномерным распределением вероятностей:
p
1
=
p
2
=
.
.
.
=
p
N
=
1
/
N
{\displaystyle p_{1}=p_{2}=...=p_{N}=1/N}.
3. Если добавить в систему состояние
p
N
+
1
=
0
{\displaystyle p_{N+1}=0}, то энтропия системы не изменится.
4. Энтропия совокупности двух систем
P
{\displaystyle P} и
Q
{\displaystyle Q} имеет вид
H
(
P
Q
)
=
H
(
P
)
+
H
(
Q
/
P
)
{\displaystyle H(PQ)=H(P)+H(Q/P)}, где
H
(
Q
/
P
)
{\displaystyle H(Q/P)} — средняя по ансамблю
P
{\displaystyle P} условная энтропия
Q
{\displaystyle Q}.
Указанный набор аксиом однозначно приводит к формуле для энтропии Шеннона.
Некоторые авторы[18] обращают внимание на неестественность последней аксиомы Хинчина. И действительно, более простым и очевидным является требование аддитивности энтропии для независимых систем. Таким образом, последняя аксиома может быть заменена следующим условием.
4'. Энтропия совокупности двух независимых систем
P
{\displaystyle P} и
Q
{\displaystyle Q} имеет вид
H
(
P
Q
)
=
H
(
P
)
+
H
(
Q
)
{\displaystyle H(PQ)=H(P)+H(Q)}.
Оказывается, система аксиом с пунктом 4' приводит не только к энтропии Шеннона, но и к энтропии Реньи.
f-энтропия
Кроме энтропии Реньи, известны и другие обобщения стандартной энтропии Шеннона, например класс f-энтропий, предложенный[19] И. Чисаром в 1972 г. Также С. Аримото в 1971 г. предложил[20] концепцию f-энтропии, задающую иной класс функционалов. Далее рассматривается концепция И. Чисара. Понятие f-энтропии связано[21] с понятием f-дивергенции. Элементы этих классов образуют парное соответствие, причём каждая такая пара функционалов определяется некоторой выпуклой функцией
f
(
t
)
{\displaystyle f(t)} при
t
≥
0
{\displaystyle t\geq 0}, удовлетворяющей условию
f
(
1
)
=
0
{\displaystyle f(1)=0}.
Для заданной функции
f
(
t
)
{\displaystyle f(t)} f-энтропия дискретного распределения
P
=
{
p
i
|
i
=
1
,
.
.
.
,
N
}
{\displaystyle P=\{p_{i}\,|\,i=1,...,N\}} определяется как
H
f
(
P
)
=
−
∑
i
=
1
N
f
(
p
i
)
.
{\displaystyle H_{f}(P)=-\sum _{i=1}^{N}f\left(p_{i}\right).}
Наиболее известными частными случаями f-энтропии являются:
энтропия Шеннона для
f
(
t
)
=
t
ln
⁡
t
{\displaystyle f(t)=t\ln t};
энтропия Берга для
f
(
t
)
=
−
ln
⁡
t
{\displaystyle f(t)=-\ln t};
энтропия Цаллиса для
f
(
t
)
=
t
q
−
t
q
−
1
{\displaystyle f(t)={t^{q}-t \over q-1}},
q
>
0
{\displaystyle q>0},
q
≠
1
{\displaystyle q\neq 1};
альфа-энтропия для
f
(
t
)
=
t
α
−
t
α
(
α
−
1
)
{\displaystyle f(t)={t^{\alpha }-t \over \alpha (\alpha -1)}},
α
≠
1
{\displaystyle \alpha \neq 1},
α
≠
0
{\displaystyle \alpha \neq 0}.
Энтропия Шеннона является единственной аддитивной энтропией в классе f-энтропий.
Понятие f-энтропии определяют в общем виде следующим образом. Пусть
P
{\displaystyle P} — распределение вероятностей и
μ
{\displaystyle \mu } — любая мера на
Ω
{\displaystyle \Omega }, для которой существует абсолютно непрерывная относительно
μ
{\displaystyle \mu } функция
p
=
d
P
d
μ
{\displaystyle p={\frac {dP}{d\mu }}}. Тогда
H
f
(
P
)
=
−
∫
Ω
f
(
p
)
∂
μ
.
{\displaystyle H_{f}(P)=-\int _{\Omega }f(p)\,\partial \mu .}
Однако непрерывные версии f-энтропий могут не иметь смысла по причине расходимости интеграла.
f-энтропия является вогнутым функционалом от распределения вероятностей.
Можно заметить, что функция
f
(
t
)
{\displaystyle f(t)} может быть задана с точностью до слагаемого
c
(
t
−
1
)
{\displaystyle c(t-1)}, где
c
{\displaystyle c} — произвольная константа. Независимо от выбора
c
{\displaystyle c} функция
f
(
t
)
{\displaystyle f(t)} порождает единственный функционал f-дивергенции. А функционал f-энтропии оказывается определённым с точностью до произвольной аддитивной постоянной, то есть выбором константы
c
{\displaystyle c} можно задать начало отсчёта энтропии. При этом возникает следующий нюанс: в случае
μ
(
Ω
)
=
∞
{\displaystyle \mu (\Omega )=\infty } константа
c
{\displaystyle c} должна выбираться так, чтобы подынтегральное выражение не содержало ненулевых постоянных слагаемых, иначе интеграл будет всегда расходиться, то есть
c
{\displaystyle c} перестаёт быть произвольной. В частности, в дискретной версии энтропии константа
c
{\displaystyle c} должна фиксироваться при
N
=
∞
{\displaystyle N=\infty }. Поэтому для f-энтропии, чтобы не уменьшать общность определения, можно явно указывать аддитивную константу. Например, если
μ
{\displaystyle \mu } — лебегова мера на
Ω
{\displaystyle \Omega }, тогда
p
(
x
)
{\displaystyle p(x)} — плотность распределения вероятности и
H
f
(
P
)
=
−
∫
Ω
f
(
p
(
x
)
)
d
x
+
c
,
{\displaystyle H_{f}(P)=-\int _{\Omega }f(p(x))\,dx+c,}
где
c
{\displaystyle c} — произвольная константа.
Функция
f
(
t
)
{\displaystyle f(t)} может также задаваться с точностью до произвольного положительного сомножителя, выбор которого равносилен выбору единицы измерения соответствующей f-энтропии или f-дивергенции.
Сравнивая выражения для f-энтропии и f-дивергенции в общем виде, можно записать следующее связывающее их соотношение[22]:
H
(
P
)
=
−
D
(
P
∥
Q
0
)
+
c
,
{\displaystyle H(P)=-D(P\parallel Q_{0})+c,}
где
Q
0
{\displaystyle Q_{0}} — равномерное на
Ω
{\displaystyle \Omega } распределение. Если положить, что аргументами энтропии и дивергенции выступают производные распределений по мере
μ
{\displaystyle \mu } (то есть плотности распределений), имеет место формальная запись
H
(
p
)
=
−
D
(
p
∥
1
)
+
c
.
{\displaystyle H(p)=-D(p\parallel 1)+c.}
Данная связь носит фундаментальный характер и играет важную роль не только в классах f-энтропии и f-дивергенции. Так, данное соотношение справедливо для энтропии и дивергенции Реньи и, в частности, для энтропии Шеннона и дивергенции Кульбака—Лейблера. Обусловлено это тем, что согласно общепринятой аксиоматике энтропия достигает максимума на равномерном распределении вероятностей.
В биологии
Вводимая обычно как «мера неупорядоченности или неопределенности системы» энтропия часто используется в рассуждениях о направленности эволюционных процессов. Согласно этой точке зрения, биосфера — сверхсложная самоорганизующаяся структура, «питающаяся» неограниченной энтропией солнечного излучения[23][24]. Бактериородопсин выполняет ту же функцию, что и хлорофилл (туннельный эффект) — обеспечивает преобразование электромагнитного облучения в энергию химических связей. Если говорить о порядке, то упорядочивание расположения элементов фотосинтетической электрон-транспортной цепи обеспечивается фотосинтетической мембраной (структурной единицей хлоропластов), которая определяет направленный перенос электронов и протонов, создавая и поддерживая разность электрохимических потенциалов ионов, разделяя окисленные и восстановленные продукты и препятствуя их рекомбинации[25].
Считается, что сложность организации по-разному влияет на устойчивость в живой и неживой природе[26][27]. В неживой природе увеличение сложности приводит к понижению устойчивости живого вещества. В противоположность этому в живой природе сложные (социальные) организации устойчивее (в смысле способности к выживанию), нежели устойчивость каждого элемента в отдельности. Например, численность организмов, состоящих из малого числа клеток (например, москитов), значительно больше численности организмов, состоящих из большого числа клеток (например, слонов). Однако это ничего не говорит об устойчивости, отнесенной к элементарной составляющей. Если бы цитолог пожелал заняться статистикой и собрал случайным образом коллекцию клеток, то он нашел бы в ней больше всего клеток, принадлежащих млекопитающим. Это говорит о том, что с усложнением живых организмов устойчивость их элементарных составляющих (клеток) значительно увеличивается[28].
По аналогии с шенноновским определением энтропии в качестве меры организованности можно рассматривать величину
G
=
−
∑
i
=
1
N
+
1
p
i
log
⁡
p
i
,
{\displaystyle G=-\sum _{i=1}^{N+1}p_{i}\log p_{i},}
где
p
i
{\displaystyle p_{i}} — отношение числа связей
n
i
,
{\displaystyle n_{i},} имеющихся у элемента
i
{\displaystyle i} в данный момент, к числу всех возможных
N
{\displaystyle N} связей этого элемента. Здесь, как и в случае определения энтропии источника информации, справедливо условие
0
≤
p
i
≤
1
,
{\displaystyle 0\leq p_{i}\leq 1,} однако условие
∑
i
p
i
=
1
,
{\displaystyle \sum _{i}p_{i}=1,} выполняющееся для случая определения энтропии, здесь уже не имеет места и заменяется неравенством
0
≤
∑
i
p
i
≤
(
N
+
1
)
.
{\displaystyle 0\leq \sum _{i}p_{i}\leq (N+1).} Для элемента
i
,
{\displaystyle i,} не имеющего ни одной связи с любым другим элементом,
p
i
=
0.
{\displaystyle p_{i}=0.} Напротив, когда элемент
i
{\displaystyle i} соединен со всеми другими
N
{\displaystyle N} элементами,
p
i
=
1
{\displaystyle p_{i}=1} и
log
⁡
p
i
=
0.
{\displaystyle \log p_{i}=0.}
Выражение для меры относительной организованности запишется следующим образом:
G
=
−
∑
i
=
1
N
+
1
n
i
N
log
⁡
n
i
N
=
−
1
N
∑
i
=
1
N
+
1
(
n
i
log
⁡
n
i
+
n
i
log
⁡
1
N
)
=
−
1
N
log
⁡
1
N
∑
i
=
1
N
+
1
n
i
−
1
N
∑
i
=
1
N
+
1
n
i
log
⁡
n
i
.
{\displaystyle G=-\sum _{i=1}^{N+1}{\frac {n_{i}}{N}}\log {\frac {n_{i}}{N}}=-{\frac {1}{N}}\sum _{i=1}^{N+1}(n_{i}\log n_{i}+n_{i}\log {\frac {1}{N}})=-{\frac {1}{N}}\log {\frac {1}{N}}\sum _{i=1}^{N+1}n_{i}-{\frac {1}{N}}\sum _{i=1}^{N+1}n_{i}\log n_{i}.}
Максимальная организованность находится приравниванием
G
{\displaystyle G} по всем
p
i
{\displaystyle p_{i}} нулю, в результате чего получается система из
N
+
1
{\displaystyle N+1} уравнений:
d
G
∂
p
j
=
−
d
∂
p
j
∑
i
=
1
N
+
1
p
i
log
⁡
p
i
=
0
(
j
=
1
,
(
N
+
1
)
¯
)
.
{\displaystyle {\frac {dG}{\partial p_{j}}}=-{\frac {d}{\partial p_{j}}}\sum _{i=1}^{N+1}p_{i}\log p_{i}=0\quad \quad (j={\overline {1,(N+1)}}).}
Для любого из этих уравнений справедливо
d
G
∂
p
j
=
−
d
∂
p
j
(
p
j
log
⁡
p
j
)
−
d
∂
p
j
∑
i
≠
j
p
i
log
⁡
p
i
=
−
log
⁡
e
−
log
⁡
p
j
−
0
=
0
,
log
⁡
p
j
=
−
log
⁡
e
=
log
⁡
1
e
.
{\displaystyle {\frac {dG}{\partial p_{j}}}=-{\frac {d}{\partial p_{j}}}(p_{j}\log p_{j})-{\frac {d}{\partial p_{j}}}\sum _{i\neq j}p_{i}\log p_{i}=-\log e-\log p_{j}-0=0,\quad \quad \log p_{j}=-\log e=\log {\frac {1}{e}}.}
Таким образом, для достижения максимума организованности отношение связи должно быть равно
p
i
=
1
e
{\displaystyle p_{i}={\frac {1}{e}}} (где
e
{\displaystyle e} — число Эйлера),
G
m
a
x
=
(
N
+
1
)
1
e
log
⁡
e
.
{\displaystyle G_{\mathrm {max} }=(N+1){\frac {1}{e}}\log e.}
Данное нестохастическое толкование организованности обладает и тем преимуществом, что позволяет сделать ряд интересных выводов. Для учета в степени связи наличия связи между двумя элементами через промежуточные элементы нужно будет использовать не число связей, подходящих к элементу
i
,
{\displaystyle i,} а число, которое определяется из выражения
n
i
=
∑
j
≠
i
m
i
,
j
,
{\displaystyle n_{i}=\sum _{j\neq i}m_{i,j},}
где
m
i
,
j
=
m
j
,
i
{\displaystyle m_{i,j}=m_{j,i}} — степень родства (сила связи) между элементами
i
{\displaystyle i} и
j
.
{\displaystyle j.} В этом случае
p
i
{\displaystyle p_{i}} будет представлять в формуле относительную общую силу связи (вместо числа связей, как было ранее) для элемента
i
.
{\displaystyle i.}[29]
См. также
Информационная энтропия
Энтропия Реньи
Энтропия Цаллиса
Дифференциальная энтропия
Расстояние Кульбака—Лейблера
Хаос
Закон неубывания энтропии
Энергия
Второе начало термодинамики
Принцип Ландауэра
Телеономия
Динамический хаос
Энтальпия
Примечания
↑ Показать компактно
 Spectrum 1976.
 «Энтропия» — статья в Малой советской энциклопедии; 2 издание; 1937—1947 гг.
 Саночкина Ю. В. Совершенствование методов управления инновационными процессами в экономических системах. — СПб.: [[ПЕТРОПОЛИС (издательство)|]], 2020. — С. 54—55. — 160 с. — ISBN 978-5-9676-1219-0.
 Зубарев Д. Н., Морозов В. Г. Диссипация энергии // Физическая энциклопедия : [в 5 т.] / Гл. ред. А. М. Прохоров. — М.: Советская энциклопедия (т. 1—2); Большая Российская энциклопедия (т. 3—5), 1988—1999. — ISBN 5-85270-034-7.
 Сивухин Д. В. Общий курс физики. — М., 1979. — Т. II. Термодинамика и молекулярная физика. — С. 127.
 Шамбадаль П. Развитие и приложение энтропии, 1967, с. 61—64.
 Цыпкин Я. З., 1995, с. 77.
 Зубарев Д. Н., Морозов В. Г. Энтропия // Физическая энциклопедия : [в 5 т.] / Гл. ред. А. М. Прохоров. — М.: Советская энциклопедия (т. 1—2); Большая Российская энциклопедия (т. 3—5), 1988—1999. — ISBN 5-85270-034-7.
 Энтропия // Большая советская энциклопедия : [в 30 т.] / гл. ред. А. М. Прохоров. — 3-е изд. — М. : Советская энциклопедия, 1969—1978.
 Источник. Дата обращения: 1 марта 2017. Архивировано из оригинала 1 марта 2017 года.
 Источник. Дата обращения: 19 марта 2017. Архивировано 19 марта 2017 года.
 Вентцель Е. С., 1969, с. 468—475.
 Зарипов Р. Г., 2005, с. 13—22, 108-125.
 Джейнс Э. Т. О логическом обосновании методов максимальной энтропии // ТИИЭР. — 1982. — Т. 70, вып. 9. — С. 33—51.
 Колмогоров, 1987, с. 29—39.
 Amritansu Ray, S. K. Majumder. A Note on Burg’s Modified Entropy in Statistical Mechanics // Mathematics. — 2016. — Т. 10, вып. 4.
 Хинчин А. Я. Понятие энтропии в теории вероятностей // Успехи математических наук. — 1953. — Т. 8, вып. 3(55). — С. 3—20. Архивировано 10 сентября 2009 года.
 Plastino A., Plastino A. R. Tsallis Entropy and Jaynes' Information Theory Formalism // Brazilian Journal of Physics. — 1999. — Т. 29, вып. 1. — С. 53. Архивировано 12 мая 2021 года.
 Csiszár I. A class of measures of informativity of observation channels. // Periodica Math. Hungar. — 1972. — Т. 2. — С. 191–213.
 Arimoto S. Information-theoretical considerations on estimation problems // Information and Control. — 1971. — Т. 19, вып. 3. — С. 181–194. Архивировано 6 декабря 2021 года.
 Csiszár I. Axiomatic Characterizations of Information Measures. // Entropy. — 2008. — Вып. 10. — С. 261—273. Архивировано 22 сентября 2017 года.
 Cichocki A., Amari S.-I. Families of Alpha- Beta- and Gamma divergences: Flexible and robust measures of similarities. // Entropy. — 2010. — Т. 12, вып. 6. — С. 1532–1568. Архивировано 23 сентября 2017 года.
 Рапапорт А. — Математические аспекты абстрактного анализа систем // Исследования по общей теории систем. М.: Прогресс. 1969. С. 83-105.
 Н. Н. Брушлинская, Фактор-инвариантность уравнений химической кинетики вдоль одномерного множества в пространстве параметров, УМН, 1975, том 30, выпуск 6(186), 161—162.
 Кадошников С. И. — Фотоэлектрические и спектральные свойства искусственных хлорофилл-липидных мембран.
 Усков А. А., Круглов В. В. — Устойчивость больших систем.
 George J.Klir — Architecture of systems problem solving.
 Г.Фёрстер — Био-логика // «Проблемы бионики: Биологические прототипы и синтетические системы», изд. «Мир», М., 1965.
 Р.Бойелл — Память с семантическими связями // «Проблемы бионики: Биологические прототипы и синтетические системы», изд. «Мир», М., 1965.
Литература
В родственных проектах

Цитаты в Викицитатнике

Медиафайлы на Викискладе
к.т.н. Деменок С. Л. Просто Энтропия. — Цикл изданий "Фракталы и Хаос". — СПб.: «СТРАТА», 2019.
Шамбадаль П. Развитие и приложение понятия энтропии. — М.: Наука, 1967. — 280 с.
Мартин Н., Ингленд Дж. Математическая теория энтропии. — М.: Мир, 1988. — 350 с.
Хинчин А. Я. Понятие энтропии в теории вероятностей // Успехи математических наук. — 1953. — Т. 8, вып. 3(55). — С. 3—20.
Гленсдорф П., Пригожин И. Термодинамическая теория структуры, устойчивости и флуктуаций. — М., 1973.
Пригожин И., Стенгерс И. Порядок из хаоса. Новый диалог человека с природой. — М., 1986.
Брюллюэн Л. Наука и теория информации. — М., 1960.
Винер Н. Кибернетика и общество. — М., 1958.
Винер Н. Кибернетика или управление и связь в животном и машине. — М., 1968.
Де Гроот С., Мазур П. Неравновесная термодинамика. — М., 1964.
Зоммерфельд А. Термодинамика и статистическая физика. — М., 1955.
Петрушенко Л. А. Самодвижение материи в свете кибернетики. — М., 1974.
Эшби У. Р. Введение в кибернетику. — М., 1965.
Яглом А. М., Яглом И. М. Вероятность и информация. — М., 1973.
Волькенштейн М. В. Энтропия и информация. — М.: Наука, 1986. — 192 с.
Вентцель Е. С. Теория вероятностей. — М.: Наука, 1969. — 576 с.
Зарипов Р. Г. Новые меры и методы в теории информации. — Казань: Изд-во Казан. гос. техн. ун-та, 2005. — 364 с.
Цыпкин Я. З. Информационная теория идентификации. — М.: Наука. Физматлит, 1995. — 336 с.
Колмогоров А. Н. Теория информации и теория алгоритмов. — М.: Наука, 1987. — 304 с.
Ссылки
Фракталы и Хаос — Проект издательства научно-популярной литературы «СТРАТА», СПб
Перейти к шаблону «Внешние ссылки» Перейти к элементу Викиданных  Словари и энциклопедии
Большая датская Большая каталанская Большая китайская Большая китайская Большая норвежская Большая российская (старая версия) Ларусса Britannica (онлайн) PWN Universalis Гранат
В библиографических каталогах
Перейти к шаблону «Термодинамические потенциалы»
Термодинамические потенциалы
Внутренняя энергия Энтальпия Свободная энергия Гельмгольца Энергия Гиббса Большой термодинамический потенциал
Портал «Физика»
Улучшение статьи
Для улучшения этой статьи желательно:
Найти и оформить в виде сносок ссылки на независимые авторитетные источники, подтверждающие написанное.
Проставить сноски, внести более точные указания на источники.
После исправления проблемы исключите её из списка. Удалите шаблон, если устранены все недостатки.
Категории: Теория хаосаТермодинамические потенциалыРелятивистские инвариантыЭнтропия
Навигация
Вы не представились системе
Обсуждение
Вклад
Создать учётную запись
Войти
СтатьяОбсуждение
ЧитатьТекущая версияПравитьПравить кодИстория
Поиск

Заглавная страница
Содержание
Избранные статьи
Случайная статья
Текущие события
Пожертвовать
Участие
Сообщить об ошибке
Как править статьи
Сообщество
Форум
Свежие правки
Новые страницы
Справка
Инструменты
Ссылки сюда
Связанные правки
Служебные страницы
Постоянная ссылка
Сведения о странице
Цитировать страницу
Получить короткий URL
Скачать QR-код
Развернуть всё
Печать/экспорт
Скачать как PDF
Версия для печати
В других проектах
Викисклад
Викицитатник
Элемент Викиданных

На других языках
العربية
Ελληνικά
English
Español
हिन्दी
Bahasa Indonesia
Latviešu
اردو
中文
Ещё 81
Править ссылки
Эта страница в последний раз была отредактирована 5 октября 2024 в 11:27.
Текст доступен по лицензии Creative Commons «С указанием авторства — С сохранением условий» (CC BY-SA); в отдельных случаях могут действовать дополнительные условия. Подробнее см. Условия использования.
Wikipedia® — зарегистрированный товарный знак некоммерческой организации «Фонд Викимедиа» (Wikimedia Foundation, Inc.)
Политика конфиденциальностиОписание ВикипедииОтказ от ответственностиСвяжитесь с намиКодекс поведенияРазработчикиСтатистикаЗаявление о кукиМобильная версия
Wikimedia FoundationPowered by MediaWiki